It has been planned to integrate various features together using the libraries for sentiment analysis and go forward about the approach.
Various Steps planned are:
  •	For any random text, identify the sentiments of the text and assign it with a sentiment score based on how positive or otherwise negative the text is.
  •	Once the sentiment score has been calculated we look up for the text intent using n grams and tokenization.
  •	N gram approach breaks the texts into positive grams and tokenization is removing irrelevant one two letter words from the text.
  •	Sarcasm is when there is negation of a negative statement or when one meaning is negated in the other token of the sentences. For           finding whether this has been the case or finding any case, we use the NLTK dictionary to get the near exact if not exact meaning         and then give sentiment scores to parts of the texts or tokens that have been made up. If the scores made in any part have been           exactly or nearly complemented in the other half, it confirms there is some sort of irony in the text. This can be implemented using        a better version of Naïve based Approach and applying it on the big data once the data is available.
  •	Combining the sentiment score and the returned ironic score from the above approach, we derive a formula (probabilistic) to find out          the exact measure of sorts to detect the sarcasm in the text.
  •	The data if fed to the code for finding scores of the text as to how positive some texts are, can be used to find things further.
  •	Many cases include where sarcasm cannot be accurately detected based on Just the simple meaning of the text. For such cases where some      background input is needed we ask the user to give the background details regarding the text and map it in the tokens that you break      in the desired to find out text and then give it an ironic score based on the input provided.
One can also give weightage to many other subdued factors like getting the text content sorted from various NLTK tools or if twitter tweets then based on inbuilt provided hashtags we can give the tweet auto weightage to reduce the complexity of the code to straight away detect that the text is actually sarcastic as the user said.

This finds all the possible tags of a given word (for tag evaluation) using the pre-tagged corpus and applies, if the word is the starting word of a respective sentence otherwise it applies. Next, it selects the tag whose probability value is maximum. For example: once you encounter a POS tag determiner (DT), such as ‘the’, maybe the probability that the next word is a noun is 40% and it being a verb is 20%. Once the model finishes its training, it is used to determine whether ‘can’ in ‘the can’ is a noun (as it should be) or a verb.
Parsing is a process of analyzing grammatical structure, identifying its parts of speech and syntactic relations of words in sentences. When a sentence is passed through a parser, the parser divides the sentence into words and identifies the POS tag information. With the help of the POS information and syntactic relation, it forms units like subject, verb, and object, then determines the relations between these units and generates a parse tree.
HMM uses pre-tagged American English words as an input and creates three dictionary objects, namely WT, TT and T. WT stores the number of occurrence of each word with its associated tag in the training corpus. Similarly, TT stores the number of occurrence of the bi-gram tags in the corpus and T stores the number of occurrence of unigram tag. For each word in the sentence, it checks if the word is the starting word of the sentence or not. If a word is the starting word then it assumes the previous tag to be ‘$’. Otherwise, the previous tag is the tag of the previous word in the respective sentence. It increases the occurrence of various tags through the dictionary objects WT, TT and T. Finally, it creates a probability table using the dictionary objects WT, TT and T.

All the above classifications put together in a string of common methods can be used to find if a sentence is actually sarcastic or not. The best results can ofcouse be achieved by decent machine learning algorithms work on which is still undergoing.
